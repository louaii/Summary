1.
Docker certificed associate Lab --> click on matser--> click on terminal

Execute the command to become root user:

# sudo su -


Check the docker version on the terminal:

# docker --version

Upgrade the docker version
=============================

# apt-get update
# apt install containerd -y
# apt install docker.io
# systemctl start docker
# systemctl status docker




Docker Images
=================================================

By Default we will not have images on the docker host machine.

All the images will be available at docker host

Demo1: List images on the dockerHost

# docker images


Demo 2: Pull images on the Docker Host

# docker pull ubuntu

Demo 3: pull an Image of a specific tag

# docker pull ubuntu:20.04

Demo 4: See details about image

# docker history ubuntu

# docker history ubuntu:20.04


Demo 5: Delete an Image

# docker rmi ubuntu
# docker rmi ubuntu:20.04


Docker Image carries it address form where we have to pull it

The image Name will be in the below format:

 registrypath/reponame/Imagename:tagname

docker.io/library/ubuntu:latest

Example:
===================
# docker pull ubuntu

# docker pull sonal04/myimage:01

# docker pull awsRegistry/reponame/imagename

# docker pull 172.34.56:5000/repo1/imagename

=====================================


Demo : Pull an Image and run the image to launch a container.

# docker pull ubuntu 

Run the image to launch a container

# docker run ubuntu

Command to list all the containers

# docker ps -a

Container will be created , will have a name and will have an ID and status will be exited.

Create a container from same image but give a unique name to the container:

# docker run --name cont1 ubuntu

Container will be created , will have a name as cont1 and will have an ID and status will be exited.

Note: When containers are in exited state, docker host will not allocate any CPU or memory to the containers


You can check number of resources allocated to container on the host machine using the below command

# docker stats

Press CTRL+c to come out to the terminal.


Foreground Mode(-it)
=========================
Here:

i= interactive
t=terminal

With this option:
  >  A container will be created from the Image
 > Container will be with status Running
 > User of the host machine will be attached to the terminal of the container


# docker run --name u1 -it ubuntu

Come out of the container and keep it running

# press key CTRL pq

Check status of container:

# docker ps -a

Again attach to the terminal of the Up and running container

# docker attach containername/id

Example: docker attach 525ee7990ca0

Note: You can attach only to the up and running OS image containers


Comeout of the container by giving exit signal

# exit


You will be on the VM and container status will be Exited.


Start an exited container:
=======================

# docker start containername/id

# docker ps -a

====================================
Stop a running container 

# docker stop containername

Kill a running container

# docker kill containername

==================================
Delete containers on the docker host

# docker rm -f $(docker ps -aq)

===================================

Detached Mode: (-d)
===================================
With this option:
  >  A container will be created from the Image
 > Container will be with status Running
 > User of the host machine will be detached from the container

# docker run --name web -d nginx

How can we access or attach to the terminal of above container:

# docker exec web uname

Example: docker exec contaienrname command

Docker execute a command on the container.

How can we access or attach to the terminal of nginxcontainer:

# docker exec -it web bash

Give exit to comout of the command


Container will still be running

# docker ps


Check logs of the container:
==============================

# docker logs containername


Describe the details of the container:
=========================

# docker inspect containername/id


Delete all images and stopped containers

# docker system prune --all

Give y 
For command to complete.

All stopped container will be deleted
All images that are dangling(not associated to a container) will be deleted

====================================
2.
# sudo su -

Run this command to delete all the containers

# docker rm -f $(docker ps -aq)

Detached Mode: (-d)
===================================
With this option:
  >  A container will be created from the Image
 > Container will be with status Running
 > User of the host machine will be detached from the container

# docker run --name web -d nginx


How can we access or attach to the terminal of above container:

# docker exec web uname

Example: docker exec contaienrname command

Docker execute a command on the container.

How can we access or attach to the terminal of nginxcontainer:

# docker exec -it web bash

Give exit to comout of the command


Container will still be running

# docker ps


Check logs of the container:
==============================

# docker logs containername


Describe the details of the container:
=========================

# docker inspect containername/id


Delete all images and stopped containers

# docker system prune --all

Give y 
For command to complete.

All stopped container will be deleted
All images that are dangling(not associated to a container) will be deleted

====================================

Port Forwarding or Port mapping

It is implemented using the flag 

-p or -P

In case of -p, docker admin has to provide the port details

System port : a port available on VM
Target port: application port exposed on the container


-p systemport:targetport

***
Port mapping in docker will be done at runtime
Once a container is created we will not do port mapping

Demo:

Create a container with image nginx and do the port mapping

# docker run -d --name web1 -p 8484:80 nginx

# docker ps -a

Access the container application from the browser:

Go to Practice labs → click on Master→ select desktop

Open the FF browser and give localhost:8484

We will see nginx
==================
OR

Port mapping using flag -P

In this cases docker will take a random port and map it to target port

# docker run -d –name web3 -P httpd


Docker Volumes:
===============================

To check if volume is present or not:

# docker volume ls

To create a volume

# docker volume create myvol

See details about the volume:

# docker inspect myvol

Demo:
Map the volume to the container directory and check if container data is preserved in the volume or not.

# docker run -it --name c2 -v myvol:/tmp  ubuntu

You will be on the container

# cd /tmp

Create 2 files

# touch contfile1 contfile2
Come out of container

# CTL pq

Go to volume directory:

# cd /var/lib/docker/volumes/myvol/_data

# ls

Container files will be present

delete the container

# docker rm -f $(docker ps -aq)

Still the files will be on the volume.

====================
Assignment:
Demo 2: Sharing of data between containers


Volume -> Bind Mount
==================================
This is second type of volume
In this volume we don't preserve the data of the container in the docker area(/var/lib/docker/volumes)
Rather we preserve containers data in any directory of the host machine
There is no name given to the volume
When we do the volume mapping, the host directory path is mapped to container directory path

Example :  

  -v  /home/labuser/mydata:/tmp


Demo:
—----------------------------

We have a GITHUB repo with HTML code, we have to copy the code on the container directory

Repo path : https://github.com/Sonal0409/ecomm.git

Step 1:

# git clone https://github.com/Sonal0409/ecomm.git

# cd ecomm

# docker run -d --name myweb -P -v /root/ecomm:/usr/local/apache2/htdocs/  httpd


Commiting a container into an Image
====================================

Build an Image from a Container

Create a container:
# docker run -it –name cont1 ubuntu

On the container, run some commands:

# apt-get update && apt-get install git -y

# touch index.html

Come out of the container

# CTL pq

Commit a container into an Image

command: docker commit containerName Imagename

# docker commit cont1 myimage01

# docker images

3.
Dockerfile
=======================================
> It is a simple test file
> the name of the file is always Dockerfile or dockerfile
> In this file we write instructions of what we need on the container
Dockerfile → build into an Image→ run into Container

In dockerfile, we write only 2 things

Keyword and 2. Argument

These keywords are given to you by docker
Argument is user provided command or script

Syntax:
================

Keyword argument

We can maintain dockerfile in the version control system

Dockerfile keywords and details
=========================

1. FROM

With this keyword we will give the Base image name of docker
Whenever we are creating our custom image, it will always be based upon a docker base image
We take a base image and do customizations on it.
This keyword will not be repeated.
This keyword will always be the first line of dockerfile

2. RUN

With this keyword we will give the linux commands that will install/upgrade/remove packages
we will give the linux commands to create directories, files, users
we will give the linux commands to run a script
This keyword can be repeated multiple times
These command given with run keyword are executed on the container and completed then the container is launched.
When the image is run, the container comes with pre installed packages, users, directories given with run commands


3. COPY : This keyword is used to copy any file on the containers directory
This keyword cannot copy tar files

4. ADD : This keyword is used to copy any file on the containers directory
But ADD keyword is specially used when we have to copy TAR files on Containers directory

5. EXPOSE : This keyword is used in order to expose the container to a port number. 
So that we can access the application running on the container.
This port number will then be used during port mapping at runtime.

6. CMD : with this keyword we will give a command/arguments that will get executed 
when the container is launched. 
Once is container is launched what is final command that  will be executed on the container.

In this keyword, if you have requirement to change the final CMD command, 
you are allowed to change it.
The docker admin can replace the CMD command coming from the image by passing a new command at runtime


7. ENTRYPOINT :
with this keyword we will give a command/arguments that will get executed 
when the container is launched. 
Once is container is launched what is final command that  will be executed on the container
With this keyword, we cannot replace the final command unlike CMD keyword ,
here the docker admin can pass a new command at runtime, but the new command/argument 
will get appended with the command that is coming from dockerfile.

8. ENV : This keyword is used to store repeated data of the dockerfile.
It is used to create a variable and store value into it.
This variable can then be used in the dockerfile.

9 WORKDIR

10 VOLUME

11 LABEL



Demo:
=======================

# sudo su -

# mkdir mydockerfiles

# cd mydockerfiles

==============================
Demo 1: 

# vim app.py

from flask import Flask 
import os 
app = Flask(__name__) 
@app.route('/') 

def hello(): 
    return ('\nHello from Container World! \n\n')

if __name__ == "__main__": 
    app.run(host="0.0.0.0", port=8080, debug=True)


Save the file

# vim dockerfile

FROM ubuntu:20.04
RUN apt update && apt install python3 -y && apt install python3-flask -y
COPY app.py /tmp
EXPOSE 8080
CMD ["python3", "/tmp/app.py"]


save the file

# docker build -t myimage01 .

# docker run -d -P myimage01


Dockerfile Demo
====================================

# git clone https://github.com/Sonal0409/nodejsappDockerfile.git

# cd nodejsappDockerfile

# docker build -t nodeapp .

# docker run -d -P nodeapp:latest

Access the app from the browser : http://localhost:49153


========================================

# vim index.html

Press i and insert text in it
This file is form docker.

Save the file.

# vim dockerfile

Press i and insert below content

FROM ubuntu
RUN apt-get update
ENV pkg_name nginx
ENV dest_path /var/www/html/
RUN apt-get install $pkg_name -y
COPY index.html $dest_path
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]


Save the file 

# docker build -t appdeploy .

Run the image to launch the container

# docker run -d --name cont1 -P  appdeploy

4.
Docker Networking:
======================

# sudo su -

# docker rm -f $(docker ps -aq)

# docker network ls

Create a custom bridge network

# docker network create --driver bridge net1

Create one more custom network:

# docker network create --driver bridge net2

# docker network ls

 place the container in the custom network

# docker run -itd --network net1 --name cont1 busybox
# docker inspect cont1

Create one more container in the same custom network:

# docker run -itd --network net1 --name cont2 busybox

Attach to anyone container:

# docker attach cont1

# ping cont2

Containers can communicate using hostnames.

Create one more container in the different custom network:

# docker run -itd --network net2 --name cont3 busybox

Attach to anyone container:

# docker attach cont3

# ping cont1

# ping cont2


We will see that cont3 cannot ping cont1 or cont2

Container can communicate within same network only


Docker-Compose Tool:
============================

 Whenever we have to deploy multiple container with a single command we will use docker-compose tool

This tool used a file with name as docker-compose.yml

This file is written in YAML which is declarative nature

In the compose file we will  declare below details for multiple microservices, 
Image details
Port details
Volume details
Network details
Command details

When the compose file is executed, it will pull the image and create containers one after the other

Using this tool, we can created dependency between containers

Using this tool, we will be able to create multi container and delete all containers

Docker-compose when executed creates a custom bridge network due to which all the container created by docker-compose tool can communicate with each other using container hostname

YAML:
=====================

YAML stands for Yet another markup langauge or YAML aint a Mark up langauge

YAML is not programming or scripting language.

It is just a format of saving data in a file.

In YAML we save data as key and value pair
A key can store single value or a list of values
The file with YAML code should have an extension of yml or YAML

while writing YAML indentation is mandatory.

Syntax in YAML file:
key: value 

Value can be a:
 - String
 - Integer
 - decimal
 - Boolean

The key will always be given to us by the tool
- keys are tool specific 

Value is always given by the admin/user 

a key: value  ==> Map 

Example 1: A key storing single value 

---
company: Simplilearn
Trainer: SonalMittal
Training: Docker 
Days: Weekdays
Time: 10AM
...

Example 2: A key storing list of values 

---
company: Simplilearn
Trainers: 
  - SonalMittal
  - Ravi
  - John
  - Rahul
Trainings: 
  - Docker 
  - kubernetes
  - Ansible
  - Jenkins
Days: 
  - Weekdays
  - Weekends
Time: 10AM
...

Example 3: A key storing list of Maps  

---
company: Simplilearn
Trainers: 
  - name: SonalMittal
    Email: Sonal@gmail.co
    phone: 3535446547
  - name: Ravi
    email: ravi@gmail.co
    phone: 43534534546



Install docker-compose tool:
*******************************************

curl -SL https://github.com/docker/compose/releases/download/v2.13.0/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose

sudo chmod +x /usr/local/bin/docker-compose

docker-compose --version


Demo for docker-compose:

# mkdir mycomposefiles
# cd mycomposefiles
# vim docker-compose.yml

version: '3'

services:
 db:
  image: mysql:5.7
  volumes:
   - db_data:/var/lib/mysql
  restart: always
  environment:
   MYSQL_ROOT_PASSWORD: password
   MYSQL_DATABASE: wordpress
   MYSQL_USER: wordpress
   MYSQL_PASSWORD: wordpress
  networks:
   - wpsite
 phpmyadmin:
  depends_on:
   - db
  image: phpmyadmin/phpmyadmin
  restart: always
  ports:
   - '8181:80'
  environment:
   PMA_HOST: db
   MYSQL_ROOT_PASSWORD: password
  networks:
   - wpsite
 Wordpress:
  depends_on:
   - db
  image: wordpress
  ports:
   - '8000:80'
  restart: always
  volumes: ['./:/var/www/html']
  environment:
   WORDPRESS_DB_HOST: db:3306
   WORDPRESS_DB_USER: wordpress
   WORDPRESS_DB_PASSWORD: wordpress
   WORDPRESS_DB_NAME: wordpress
  networks:
   - wpsite
networks:
 wpsite:
volumes:
 db_data:



# docker-compose up -d
# docker-compose ps

GO to the browser of the desktop

 Give  http://localhost:8000

On another browser give -    http://localhost:8181


# docker-compose down

5.
Docker swarm
===================================================

# hostname MANAGER
# sudo su -

# docker swarm init

The current machine is now Leader machine

# copy the token which will look like this:

  docker swarm join --token SWMTKN-1-64dczervns37sw84jlb79fczskofbx9wh2x25vrv8x95s9o03m-75s49518smp5hxyyort6k8ing 172.31.41.114:2377


Go to Worker1  VM on the lab

# sudo hostname WORKER1

# sudo su -

Paste the token

The machine will join the swarm cluster

Go to worker 2  VM on the lab

# sudo hostname WORKER2

# sudo su -

Paste the token

The machine will join the swarm cluster


Go to manager node

# docker node ls

You will see all the 3 nodes part of cluster




Demo 1: 

Create a service that will create 6 replicas of the Image nginx
The service will also distribute the replicas on the nodes of the Cluster 

# docker rm -f $(docker ps -aq)

# docker service create --name mysvc --replicas 6 nginx

# docker service ls

# docker service ps mysvc

Demo 2: Ensure that the desired count of replicas are always running in the cluster


Go to any worker node, delete the replica

# docker ps -a

# docker rm -f conatinerid


On the manager node , check the service

# docker service ls


The desired count matches the current count

Orchestration tool recreates a new replica to match current count and desired count.

=============================

Scaling in the swarm cluster

If we have created a service and we want to increase the desired count of replicas or reduce the desired count of replicas  the we use the docker service scale command

In docker swarm we cannot do autoscaling.

Scale up command:

# docker service scale mysvc=7

Scale down:

# docker service scale mysvc=2


Delete the service , all the replicas also get deleted

# docker service rm mysvc
=====================================================

Swarm Service - GLOBAL
====================================================

This type of service will:

Create 1 container on each VM that part of the cluster
We have 2 VMs in our cluster, so the service of type global will create 1 replicas for each VM
You cannot increase or reduce the count of replicas
It  will always ensure 1 replica of the image is running on the VM.
Whenever a new VM is added to the cluster, the service of type global will create 1 replica on the new Vm also.

# docker service create --name mysvc --mode global nginx

=========================================

Create Service in docker swarm cluster using custom image

# docker service rm mysvc

# docker service create --name mysvc -p 8181:3000 --replicas 4 sonal04/samplepyapp:v1

To access the replicas fo to worker 1 terminal and run the below command

# while true;do curl http://localhost:8181;sleep 1;echo " ";done

You will observe the request being forwarded to different replicas which are endpoints of the service- mysvc .

Scale up and scale down the service:

# docker service scale mysvc=6

# docker service scale mysvc=3

Rolling update:
==========================

Update the image of the service:

# docker service update --image sonal04/samplepyapp:v2 mysvc

Rollback to previous version of image:

# docker service rollback mysvc

===============================================

Fail Over Scenarios:
================================

Scenario 1: Drain the Worker 1

 > no new containers will be scheduled on this worker node
> Existing containers will be recreated on the other nodes of the clsuter

# docker node ls


# docker node update –availability drain <nodeHostname>

# docker service ps mysvc | grep Running

Create a new service, no new replicas will be scheduled on the worker node that is drained

# docker service create --name mysvc2 --replicas 2 nginx

# docker service ps mysvc2 | grep Running

Bring back the service to Active status

# docker node update --availability Active  <hostname>

===========================
Scenario 2:
===========================

# docker service rm mysvc mysvc2

Create replicas:

# docker service create --name mysvc -p 8181:3000 --replicas 4 sonal04/samplepyapp:v1

# docker service ps mysvc

# docker service create --name mysvc2 --replicas 4 nginx

# docker service ps mysvc2


GO to worker 1 → leave the swarm

# docker swarm leave 


GO to Manager node

# docker node ls  → you will see worker node is with status Down

You delete the node as it is not part of the cluster.

# docker node ls

# docker node rm <nodeid>


Observe: All the replicas will be on manager and worker2 node

# docker service ps mysvc | grep Running

# docker service ps mysvc2 | grep Running


Join the worker to the swarm:

On the manager node run the below command:

# docker swarm join-token worker

Copy the generated token on worker 1

Worker will join the swarm.

6.
=========================================

Create Service in docker swarm cluster using custom image

# docker service rm mysvc

# docker service create --name mysvc -p 8181:3000 --replicas 4 sonal04/samplepyapp:v1

To access the replicas fo to worker 1 terminal and run the below command

# while true;do curl http://localhost:8181;sleep 1;echo " ";done

You will observe the request being forwarded to different replicas which are endpoints of the service- mysvc .

Scale up and scale down the service:

# docker service scale mysvc=6

# docker service scale mysvc=3

Rolling update:
==========================

Update the image of the service:

# docker service update --image sonal04/samplepyapp:v2 mysvc

Rollback to previous version of image:

# docker service rollback mysvc


===================
Docker Stack:
=====================================================

On the Manager Node

# vim myapp.yml

version: "3"
services:

  redis:
    image: redis:alpine
    networks:
      - frontend
    deploy:
      replicas: 1
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
  db:
    image: postgres:9.4
    volumes:
      - db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: "db"
      POSTGRES_HOST_AUTH_METHOD: "trust"  
    networks:
      - backend
    deploy:
      placement:
        constraints: [node.role == manager]
  vote:
    image: dockersamples/examplevotingapp_vote:before
    ports:
      - 5000:80
    networks:
      - frontend
    depends_on:
      - redis
    deploy:
      replicas: 2
      update_config:
        parallelism: 2
      restart_policy:
        condition: on-failure
  result:
    image: dockersamples/examplevotingapp_result:before
    ports:
      - 5001:80
    networks:
      - backend
    depends_on:
      - db
    deploy:
      replicas: 1
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure

  worker:
    image: dockersamples/examplevotingapp_worker
    networks:
      - frontend
      - backend
    depends_on:
      - db
      - redis
    deploy:
      mode: replicated
      replicas: 1
      labels: [APP=VOTING]
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s
      placement:
        constraints: [node.role == manager]

  visualizer:
    image: dockersamples/visualizer:stable
    ports:
      - "8080:8080"
    stop_grace_period: 1m30s
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    deploy:
      placement:
        constraints: [node.role == manager]

networks:
  frontend:
  backend:

volumes:
  db-data:


# docker stack deploy -c myapp.yml myvotingapp

# docker service ls

Go to the desktop -> firefox browser

localhost:5000 → to vote
localhost:5001 → to see results


==================================================

Project deployment on Kubernetes cluster

ON Master Node:
===========================
# kubeadm init --ignore-preflight-errors=all

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config



# kubectl get nodes


Install Container network interface

# kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

# kubectl get nodes


Generate token on master node:

kubeadm token create --print-join-command


Copy token on worker nodes.

On the master node execute

# kubectl get nodes

All the workers will be part of cluster now

Kubernetes cluster is now ready.



Execute commands on master node. No kubernetes commands run on Worker node.

==================================================

Project implementation for DCA module:
=================================================

# mkdir myproject
# cd myproject

# vim dockerfile


FROM alpine

RUN apk add --update redis

EXPOSE 6379
CMD ["redis-server"]


# docker build -t myredisproject .

# docker images




Step 2: Publish Image to docker Hub

# docker login

Give docker hub user id and password


Now change the image name:

# docker tag myredisproject  dockerhubAccount/myredisproject 



# docker push sonal04/myredisproject





Once the image is ready, we will deploy the image in the cluster:


SWARM CLSUTER:

# docker service create --name projectredis --replicas 3 sonal04/myredisproject

# docker ps

Copy the container id and let us execute command in the container

# docker exec -it 1ba0a77c2204 sh

Execute below commands on the container:

# redis-cli

127.0.0.1:6379> ping
PONG

127.0.0.1:6379> set name Docker
OK
127.0.0.1:6379> get name 
"Docker"
127.0.0.1:6379> set day Wednesday
OK
127.0.0.1:6379> get day
"Wednesday"

7.
If you master or worker nodes are not ready
You will have to reset kubernetes on the lab

ON THE MASTER NODE:
====================================

# kubeadm reset --force


ON THE both the WORKER NODES
===========================================

# kubadm reset

give y when asked

=======================================================
ON THE MASTER NODE:
============================================================

# kubeadm init --ignore-preflight-errors=all

Execute  THE BELOW COMMANDS
===================================
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config



# kubectl get nodes


Install Container network interface

# kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

# kubectl get nodes


Generate token on master node:

kubeadm token create --print-join-command


Copy token on worker nodes.
==================================

On the master node execute

# kubectl get nodes

All the workers will be part of cluster now

Kubernetes cluster is now ready.



















Execute commands on master node. No kubernetes commands run on Worker node.

Creation of POD in kubernetes.
============================

Create a pod to deploy container of our project Image.
Image is present in docker hub

Image name: sonal04/myredisproject


# kubectl run pod1 --image sonal04/myredisproject

# kubectl get pods

# vim pod1.yml

apiVersion: v1
kind: Pod
metadata:
  name: mypod
  labels:
   app: webserver
   author: sonal
spec:
 containers:
  - name: c1
    image: sonal04/myredisproject
  - name: c2
    image: tomcat



# kubectl create pod1.yml











Deploy multiple pods of the image:
===============================

# kubectl create deployment myproject --image sonal04/myredisproject --replicas 4
# kubectl get pods

Delete a pod

# kubectl delete pod <podname>

# kubectl get pods

New pod will be created

kubectl exec -it <podname> -- sh
/ # redis-cli
127.0.0.1:6379> ping
PONG



Delete all the pods for the deployment:

# kubectl delete  deployment --all



OR  USE YAML FILE:
============================

# vim myproject.yml

apiVersion: apps/v1 # this is the name of the library that K8s woll use to create deployment object
kind: Deployment
metadata:
 name: myproject-deploy
spec:
 replicas: 3
 selector:
  matchLabels:
   app: webserver
 template: # this pod template using which replicas will be created
  metadata:
   labels:  # every replica/pod will have a unique name given by K8s
    app: webserver
  spec:
   containers:
    - name: c1
      image: sonal04/myredisproject

save the file

# kubectl create -f myproject.yml

# kubectl get all





Kubernetes -> Container orchestration tool

1. Kubernetes -> also called as K8s.
2. Kubernetes also works in a cluster mode where it is installed on one VM and it manages various worker nodes.
3. In case Kubernetes the main VM is called as Master, and the other VMs are called as worker nodes.
4. By default no containers will be scheduled on the Master node, container will always be scheduled on worker nodes
5. Kubernetes can also create containers of docker as well as other container tools like CRI-O, containerD, Rkt container etc
6. The Kubernetes should have a unique Container runtime interface(container tool)
7. Once containers have been created, Kubernetes can auto scale the containers
8. Kubernetes allows us to store the data on storage drivers outside the cluster -> Persistent Volume
9. Kubernetes supports ingress load balancers
10. Kubernetes is also available as a service on various cloud platform like AWS, GCP or Azure
11 The names of k8s service are : EKS, AKS, GKE
12. In Kubernetes we have many objects to control various workflows on Kubernetes
      to create a replicas - POD, scaleup -> replicaSet , update Image -> Deployment , statefulset, service object, daemon sets
13. We can create jobs and cron jobs in Kubernetes
14. Kubernetes also supports various deployment strategies like: Rolling update, recrete, blue and green, canary deployment
15. Kubernetes also has a GUI -> Kubernetes Dashboard
16. Kubernetes provides objects like secrets to store sensitive data and configmap to store configurations in Kubernetes cluster

